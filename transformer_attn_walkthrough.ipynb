{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers: Walking Through the Math Behind Self Attention\n",
    "\n",
    "### Briefly:<br>\n",
    "**Transformers map input to output sequences**<br>\n",
    "**Self attention assumes these sequences to have the same length**<br>\n",
    "**Captures relevance of an input token for an output token**<br>\n",
    "- Input vectors $x_1, x_2, \\ldots, x_t, \\in \\mathbb{R}^{d}$\n",
    "- Output vectors $y_1, y_2, \\ldots, y_t, \\in \\mathbb{R}^{d}$\n",
    "**Calculate output as a weighted sum**<br>\n",
    "- - - \n",
    "### Barebones (walkthrough w/o query, key, value and corresponding matrices)\n",
    "**General Equations:**<br>\n",
    "$$\\begin{aligned}\n",
    "w'_{ij}=x'_{i}x_{j}\\\\\\\\\n",
    "w_{ij}=softmax(w'_{ij})=\\frac{exp(w'_{ij})}{\\sum_{j}exp(w'_{ij})}\\\\\\\\\n",
    "y_i = \\sum_{j=1}^{t}w_{ij}x_{j},\\\\\n",
    "\\end{aligned}$$\n",
    "**Specific Case - Compute $y_{2}$:**<br>\n",
    "Let $t=4$ and $i=2$, meaning we have four input (hence output) vectors and we are calculating output for the second one. \n",
    "*Note that here $i$ denotes the index of the vector which we are computing and  $j$ denotes the set over which we sum. So, $i$ is fixed and $j$ varies from $1$ to $4$.*\n",
    "\n",
    "**Step 1)**<br>\n",
    "Obtain weight matrices $w'_{ij}$: $w'_{21}, \\quad w'_{22}, \\quad w'_{23}, \\quad w'_{24}$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w'_{21}=x'_{2}x_{1}\\\\\\\\\n",
    "w'_{22}=x'_{2}x_{2}\\\\\\\\\n",
    "w'_{23}=x'_{2}x_{3}\\\\\\\\\n",
    "w'_{24}=x'_{2}x_{4}\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Step 2)**<br>\n",
    "Normalize the weight matrices using $softmax$\n",
    "$$\\begin{aligned}\n",
    "w_{21}=\\frac{exp(w'_{21})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{22}=\\frac{exp(w'_{22})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{23}=\\frac{exp(w'_{23})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{24}=\\frac{exp(w'_{24})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "\\end{aligned}$$\n",
    "**Step 3)**<br>\n",
    "Calculate weighted sum\n",
    "$$y_2=w_{21}x_{1}+w_{22}x_{2}+w_{23}x_{3}+w_{24}x_{4}$$\n",
    "- - -\n",
    "### Enter Query, Key, Value\n",
    "Every input vector $i$ appears in three roles:\n",
    "- Calculate weights for its own output $w_{ij}$ -- *query*\n",
    "- Calculate weights for other output's weights $w_{zj}$, where $j=i$ -- *key*\n",
    "- Part of the weighted sum to calculate each $y_i$ -- *value*\n",
    "\n",
    "In the case of $x_2$, the above section showed where it played a role in computing weight matrices $w_{2j}$ and output $y_2$. If we were to demonstrate equations for computing $y_{1}, y_{3}, y_{4}$, input vector $x_i$ would play a role in computing $w_{12}, w_{32}, w_{42}$ in the three different computations separately, as well as in the relevant weighted sums. \n",
    "\n",
    "**Query:**\n",
    "- $x_{i}$ appears in calculating weights for its own output $y_i$\n",
    "- $x_2$ appears in calculating weights for its own output $y_{2}$\n",
    "\n",
    "**Key:**\n",
    "- $x_i$ appears in calculating weights for all outputs\n",
    "- $x_2$ appears in calculating weights for outputs $y_1, y_2, y_3, y_4$\n",
    "\n",
    "**Value:**\n",
    "- $x_i$ appears in the weighted sum for all outputs\n",
    "- $x_2$ appears in the weighted sum for $y_1, y_2, y_3, y_4$\n",
    "\n",
    "**$W_{q}, W_{k}, W_{v}$**<br>\n",
    "In practice we have additional trainable weight matrices $W_{q}, W_{k} \\quad \\text{and} \\quad W_{v}$ each for query, key and value. These are $d \\times d$ matrices and can be multiplied by our $d \\times 1$ input vector $x_{i}$. \n",
    "\n",
    "Hence our $d \\times 1$ vector $x_{i}$ gets transformed into three separate $d \\times 1$ vectors:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_{q}x_{i} = q_{i}\\\\\\\\\n",
    "W_{k}x_{i} = k_{i}\\\\\\\\\n",
    "W_{v}x_{i} = v_{i},\n",
    "\\end{aligned}\n",
    "$$\n",
    "which are used to compute the weight matrices $w'_{ij}$.\n",
    "\n",
    "Note that in the above section $w'_{ij}=x'_{i}x_{j}$, where $x_{i}$ played the role of query - *i.e.* helped compute weights for its own output and $x_{j}$ played the role of key - *i.e.* helped compute weights for other input's output. \n",
    "\n",
    "Now using the relevant query and key vectors, we compute:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w'_{ij} = (W_{q}x_{i})'W_{k}x_{j}\\\\\\\\\n",
    "= q'_ik_j\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $w'_{ij}$ will be $1 \\times 1$ scalar, as we multiply $(d \\times 1)' (d \\times 1) = (1 \\times d)(d \\times 1)$.\n",
    "\n",
    "From here on, weight matrix $w_{ij}$ is computed as before:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_{ij} = softmax(w'_{ij}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "but again the final output computation changes from its previous form of $y_i = \\sum_{j=1}^{t}w_{ij}x_{j}$ to\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{i} = \\sum_{j=1}^{t}w_{ij}W_{v}x_{j}\\\\\\\\\n",
    "= \\sum_{j=1}^{t}w_{ij}v_{j},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $y_i$ will also be of dimension $d \\times 1$ as $w_{ij}$ is a scalar and $v_{j}$ is of dimension $d_{1}.$\n",
    "\n",
    "**Back to Specific Case - Compute $y_{2}$:**<br>\n",
    "**Step 1)**<br>\n",
    "Obtain query vector $q_2$ using $x_2$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{2}=W_{q}x_2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "Transpose of $q_2$ will play the role of query in this case, instead of $x'_2$ used previously.\n",
    "\n",
    "**Step 2)**<br>\n",
    "Obtain key vectors $k_1, k_2, k_3, k_4$ using $x_1, x_2, x_3, x_4$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "k_1 = W_kx_1\\\\\\\\\n",
    "k_2=W_kx_2\\\\\\\\\n",
    "k_3=W_kx_3\\\\\\\\\n",
    "k_4=W_kx_4\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Step 3)**<br>\n",
    "Compute matrices $w'_{ij}: w'_{21}, \\quad w'_{22}, \\quad w'_{23}, \\quad w'_{24}$\n",
    "*Note that these matrices are exactly what we call attention scores.*\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w'_{21} = q'_2k_1\\\\\\\\\n",
    "w'_{22} = q'_2k_2\\\\\\\\\n",
    "w'_{23} = q'_2k_3\\\\\\\\\n",
    "w'_{24} = q'_2k_4\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Step 4)**<br>\n",
    "Normalize weight matrices (attention scores) using $softmax$\n",
    "$$\\begin{aligned}\n",
    "w_{21}=\\frac{exp(w'_{21})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{22}=\\frac{exp(w'_{22})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{23}=\\frac{exp(w'_{23})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{24}=\\frac{exp(w'_{24})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "\\end{aligned}$$\n",
    "**Step 5)**<br>\n",
    "Obtain value vectors $v_1, v_2, v_3, v_4$ which will be used in the weighted sum to compute $y_2$ instead of previously used $x_1, x_2, x_3, x_4$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_1 = W_{v}x_1\\\\\\\\\n",
    "v_2 = W_{v}x_2\\\\\\\\\n",
    "v_3 = W_{v}x_3\\\\\\\\\n",
    "v_4 + W_{v}x_4\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Step 6)**<br>\n",
    "Calculate weighted sum:\n",
    "$$\n",
    "\\begin{align}\n",
    "y_2 = w_{21}v_1 + w_{22}v_2 + w_{23}v_3 + w_{24}v_4\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 20:58:44.223604: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-09 20:58:44.282241: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-09 20:58:44.282316: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-09 20:58:44.282354: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-09 20:58:44.296720: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "np.random.seed(66)\n",
    "tf.random.set_seed(66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of the $y_2$ Calculation in Python\n",
    "Since in the above example have four vectors in total, we imagine we had a four-word-long sentence that we embedded as $d \\times 1$ vectors $x_1, x_2, x_3, x_4$. To make things a bit more confusing, we can let $d=1$, meaning our embedding dimension is $4$ and $x_i$ is a vector of $4 \\times 1$ dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0): Generate $x_1, x_2, x_3, x_4$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vectors are:\n",
      "x_1:\n",
      "[[-0.20886648]\n",
      " [ 0.5476398 ]\n",
      " [ 0.72447395]\n",
      " [-1.1310507 ]]\n",
      "x_2:\n",
      "[[ 0.1870852]\n",
      " [-1.3414632]\n",
      " [-1.1067361]\n",
      " [-1.3406332]]\n",
      "x_3:\n",
      "[[-1.4844604 ]\n",
      " [ 0.60378075]\n",
      " [ 0.39966342]\n",
      " [-0.7903915 ]]\n",
      "x_4:\n",
      "[[-0.39180905]\n",
      " [-1.2203115 ]\n",
      " [ 1.0298591 ]\n",
      " [-0.23658466]]\n",
      "With shapes: (4, 1), (4, 1), (4, 1), (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define embedding dimension\n",
    "d = 4\n",
    "# Randomly initialize embedded vectors\n",
    "x_1 = tf.random.normal((d,1))\n",
    "x_2 = tf.random.normal((d,1))\n",
    "x_3 = tf.random.normal((d,1))\n",
    "x_4 = tf.random.normal((d,1))\n",
    "\n",
    "print(f'Our vectors are:\\n'\n",
    "      f'x_1:\\n{x_1}\\n'\n",
    "      f'x_2:\\n{x_2}\\n'\n",
    "      f'x_3:\\n{x_3}\\n'\n",
    "      f'x_4:\\n{x_4}\\n'\n",
    "      f'With shapes: {x_1.shape}, {x_2.shape}, {x_3.shape}, {x_4.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0): Generate $W_q, W_k, W_v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our matrices are:\n",
      "W_q:\n",
      "[[ 0.0558263  -1.7287809   0.06325671  0.8925434 ]\n",
      " [ 0.44502732  2.3946028  -0.7817421  -0.5631514 ]\n",
      " [ 0.8664166   0.14898889 -1.2681427  -1.3942317 ]\n",
      " [-0.52701306  0.6796619   0.95935136 -0.8240369 ]]\n",
      "W_k:\n",
      "[[ 1.4865062   0.53539854  0.707822   -0.3496642 ]\n",
      " [-0.36981565 -0.43839103  0.06208184 -0.34790948]\n",
      " [ 1.5218863  -1.1896831   0.3180406   1.2518362 ]\n",
      " [ 0.4551281  -1.190138    1.8135393   0.6393279 ]]\n",
      "W_v:\n",
      "[[-1.9406502   0.35411984 -0.59085363 -0.40704426]\n",
      " [ 0.8450196  -0.21295227  2.219851   -0.58515114]\n",
      " [ 1.1703489   1.7554591   1.4085048  -0.09804194]\n",
      " [ 0.40726846 -0.44613966 -0.89044726  0.1120782 ]]\n",
      "With shapes: (4, 4), (4, 4), (4, 4)\n"
     ]
    }
   ],
   "source": [
    "W_q = tf.random.normal((d,d))\n",
    "W_k = tf.random.normal((d,d))\n",
    "W_v = tf.random.normal((d,d))\n",
    "\n",
    "print(f'Our matrices are:\\n'\n",
    "      f'W_q:\\n{W_q}\\n'\n",
    "      f'W_k:\\n{W_k}\\n'\n",
    "      f'W_v:\\n{W_v}\\n'\n",
    "      f'With shapes: {W_q.shape}, {W_k.shape}, {W_v.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1): Obtain query vector $q_2$ using $x_2$**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{2}=W_{q}x_2,\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_2:\n",
      "[[ 1.0629584]\n",
      " [-1.5088519]\n",
      " [ 3.2348833]\n",
      " [-0.9673554]]\n",
      "With shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Multiply 4x4 W_q with 4x1 x_2\n",
    "q_2 = tf.matmul(W_q, x_2)\n",
    "\n",
    "print(f'q_2:\\n{q_2}\\n'\n",
    "      f'With shape: {q_2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2) Obtain key vectors $k_1, k_2, k_3, k_4$ using $x_1, x_2, x_3, x_4$**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "k_1 = W_kx_1\\\\\n",
    "k_2=W_kx_2\\\\\n",
    "k_3=W_kx_3\\\\\n",
    "k_4=W_kx_4\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our key vectors are:\n",
      "k_1:\n",
      "[[ 0.8910108 ]\n",
      " [ 0.27564165]\n",
      " [-2.154867  ]\n",
      " [-0.1560781 ]]\n",
      "k_2:\n",
      "[[-0.75471485]\n",
      " [ 0.91660917]\n",
      " [-0.14960161]\n",
      " [-1.1825396 ]]\n",
      "k_3:\n",
      "[[-1.3241341]\n",
      " [ 0.5840812]\n",
      " [-3.839819 ]\n",
      " [-1.174716 ]]\n",
      "k_4:\n",
      "[[-0.42409754]\n",
      " [ 0.8261164 ]\n",
      " [ 0.8868669 ]\n",
      " [ 2.9904504 ]]\n",
      "With shapes: (4, 1), (4, 1), (4, 1), (4, 1)\n"
     ]
    }
   ],
   "source": [
    "k_1 = tf.matmul(W_k, x_1)\n",
    "k_2 = tf.matmul(W_k, x_2)\n",
    "k_3 = tf.matmul(W_k, x_3)\n",
    "k_4 = tf.matmul(W_k, x_4)\n",
    "\n",
    "print(f'Our key vectors are:\\n'\n",
    "      f'k_1:\\n{k_1}\\n'\n",
    "      f'k_2:\\n{k_2}\\n'\n",
    "      f'k_3:\\n{k_3}\\n'\n",
    "      f'k_4:\\n{k_4}\\n'\n",
    "      f'With shapes: {k_1.shape}, {k_2.shape}, {k_3.shape}, {k_4.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3) Compute attention scores $w'_{ij}: w'_{21}, \\quad w'_{22}, \\quad w'_{23}, \\quad w'_{24}$**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w'_{21} = q'_2k_1\\\\\\\\\n",
    "w'_{22} = q'_2k_2\\\\\\\\\n",
    "w'_{23} = q'_2k_3\\\\\\\\\n",
    "w'_{24} = q'_2k_4\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our attention scores are:\n",
      "w_T_21:\n",
      "[[-6.288555]]\n",
      "w_T_22:\n",
      "[[-1.5252657]]\n",
      "w_T_23:\n",
      "[[-13.573791]]\n",
      "w_T_24:\n",
      "[[-1.7212026]]\n",
      "With shapes: (1, 1), (1, 1), (1, 1), (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Transpose q_2\n",
    "q_T_2 = tf.transpose(q_2)\n",
    "\n",
    "# Obtain attention scores\n",
    "w_T_21 = tf.matmul(q_T_2, k_1)\n",
    "w_T_22 = tf.matmul(q_T_2, k_2)\n",
    "w_T_23 = tf.matmul(q_T_2, k_3)\n",
    "w_T_24 = tf.matmul(q_T_2, k_4)\n",
    "\n",
    "print(f'Our attention scores are:\\n'\n",
    "      f'w_T_21:\\n{w_T_21}\\n'\n",
    "      f'w_T_22:\\n{w_T_22}\\n'\n",
    "      f'w_T_23:\\n{w_T_23}\\n'\n",
    "      f'w_T_24:\\n{w_T_24}\\n'\n",
    "      f'With shapes: {w_T_21.shape}, {w_T_22.shape}, {w_T_23.shape}, {w_T_24.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4) Normalize attention scores using $softmax$**\n",
    "$$\\begin{aligned}\n",
    "w_{21}=\\frac{exp(w'_{21})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\n",
    "w_{22}=\\frac{exp(w'_{22})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\n",
    "w_{23}=\\frac{exp(w'_{23})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\n",
    "w_{24}=\\frac{exp(w'_{24})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our normalized attention scores are:\n",
      "w_21:\n",
      "[[0.00466374]]\n",
      "w_22:\n",
      "[[0.5462668]]\n",
      "w_23:\n",
      "[[-13.573791]]\n",
      "w_24:\n",
      "[[-1.7212026]]\n",
      "With shapes: (1, 1), (1, 1), (1, 1), (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Take exponentials\n",
    "exp_w_T_21 = tf.exp(w_T_21)\n",
    "exp_w_T_22 = tf.exp(w_T_22)\n",
    "exp_w_T_23 = tf.exp(w_T_23)\n",
    "exp_w_T_24 = tf.exp(w_T_24)\n",
    "\n",
    "# Calculate sum\n",
    "exp_sum = exp_w_T_21 + exp_w_T_22 + exp_w_T_23 + exp_w_T_24\n",
    "\n",
    "# Calculate normalized scores\n",
    "w_21 = exp_w_T_21 / exp_sum\n",
    "w_22 = exp_w_T_22 / exp_sum\n",
    "w_23 = exp_w_T_23 / exp_sum\n",
    "w_24 = exp_w_T_24 / exp_sum\n",
    "\n",
    "print(f'Our normalized attention scores are:\\n'\n",
    "      f'w_21:\\n{w_21}\\n'\n",
    "      f'w_22:\\n{w_22}\\n'\n",
    "      f'w_23:\\n{w_T_23}\\n'\n",
    "      f'w_24:\\n{w_T_24}\\n'\n",
    "      f'With shapes: {w_21.shape}, {w_22.shape}, {w_23.shape}, {w_24.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5) Obtain value vectors $v_1, v_2, v_3, v_4$** \n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_1 = W_{v}x_1\\\\\n",
    "v_2 = W_{v}x_2\\\\\n",
    "v_3 = W_{v}x_3\\\\\n",
    "v_4 + W_{v}x_4\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our key vectors are:\n",
      "v_1:\n",
      "[[ 0.63159657]\n",
      " [ 1.9769425 ]\n",
      " [ 1.8482281 ]\n",
      " [-1.1012605 ]]\n",
      "v_2:\n",
      "[[ 0.3615104]\n",
      " [-1.2285581]\n",
      " [-3.5633335]\n",
      " [ 1.5099082]]\n",
      "v_3:\n",
      "[[ 3.1802108 ]\n",
      " [-0.03328285]\n",
      " [-0.03700471]\n",
      " [-1.3184092 ]]\n",
      "v_4:\n",
      "[[-0.18396774]\n",
      " [ 2.3533535 ]\n",
      " [-1.1270034 ]\n",
      " [-0.55869323]]\n",
      "With shapes: (4, 1), (4, 1), (4, 1), (4, 1)\n"
     ]
    }
   ],
   "source": [
    "v_1 = tf.matmul(W_v, x_1)\n",
    "v_2 = tf.matmul(W_v, x_2)\n",
    "v_3 = tf.matmul(W_v, x_3)\n",
    "v_4 = tf.matmul(W_v, x_4)\n",
    "\n",
    "print(f'Our key vectors are:\\n'\n",
    "      f'v_1:\\n{v_1}\\n'\n",
    "      f'v_2:\\n{v_2}\\n'\n",
    "      f'v_3:\\n{v_3}\\n'\n",
    "      f'v_4:\\n{v_4}\\n'\n",
    "      f'With shapes: {v_1.shape}, {v_2.shape}, {v_3.shape}, {v_4.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6) Calculate weighted sum $y_2$**\n",
    "$$\n",
    "\\begin{align}\n",
    "y_2 = w_{21}v_1 + w_{22}v_2 + w_{23}v_3 + w_{24}v_4\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our output vector:\n",
      "y_2:\n",
      "[[ 0.11782318]\n",
      " [ 0.39491105]\n",
      " [-2.4440105 ]\n",
      " [ 0.5687822 ]]\n",
      "With shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# * operator since attention weights are scalars\n",
    "y_2 = w_21 * v_1 + w_22 * v_2 + w_23 * v_3 + w_24 * v_4\n",
    "\n",
    "print(f'Our output vector:\\n'\n",
    "      f'y_2:\\n{y_2}\\n'\n",
    "      f'With shape: {y_2.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anbs_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
