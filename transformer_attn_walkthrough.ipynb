{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers: Walking Through the Math Behind Self Attention\n",
    "\n",
    "### Briefly:<br>\n",
    "**Transformers map input to output sequences**<br>\n",
    "**Self attention assumes these sequences to have the same length**<br>\n",
    "**Captures relevance of an input token for an output token**<br>\n",
    "- Input vectors $x_1, x_2, \\ldots, x_t, \\in \\mathbb{R}^{d}$\n",
    "- Output vectors $y_1, y_2, \\ldots, y_t, \\in \\mathbb{R}^{d}$\n",
    "**Calculate output as a weighted sum**<br>\n",
    "- - - \n",
    "### Barebones (walkthrough w/o query, key, value and corresponding matrices)\n",
    "**General Equations:**<br>\n",
    "$$\\begin{aligned}\n",
    "w'_{ij}=\\frac{x'_{i}x_{j}}{\\sqrt{d}}\\\\\\\\\n",
    "w_{ij}=softmax(w'_{ij})=\\frac{exp(w'_{ij})}{\\sum_{j}exp(w'_{ij})}\\\\\\\\\n",
    "y_i = \\sum_{j=1}^{t}w_{ij}x_{j},\\\\\n",
    "\\end{aligned}$$\n",
    "**Specific Case - Compute $y_{2}$:**<br>\n",
    "Let $t=4$ and $i=2$, meaning we have four input (hence output) vectors and we are calculating output for the second one. \n",
    "*Note that here $i$ denotes the index of the vector which we are computing and  $j$ denotes the set over which we sum. So, $i$ is fixed and $j$ varies from $1$ to $4$.*\n",
    "\n",
    "**Step 1)**<br>\n",
    "Obtain weight matrices $w'_{ij}$: $w'_{21}, \\quad w'_{22}, \\quad w'_{23}, \\quad w'_{24}$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w'_{21}=\\frac{x'_{2}x_{1}}{\\sqrt{d}}\\\\\\\\\n",
    "w'_{22}=\\frac{x'_{2}x_{2}}{\\sqrt{d}}\\\\\\\\\n",
    "w'_{23}=\\frac{x'_{2}x_{3}}{\\sqrt{d}}\\\\\\\\\n",
    "w'_{24}=\\frac{x'_{2}x_{4}}{\\sqrt{d}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Step 2)**<br>\n",
    "Normalize the weight matrices using $softmax$\n",
    "$$\\begin{aligned}\n",
    "w_{21}=\\frac{exp(w'_{21})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{22}=\\frac{exp(w'_{22})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{23}=\\frac{exp(w'_{23})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{24}=\\frac{exp(w'_{24})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "\\end{aligned}$$\n",
    "**Step 3)**<br>\n",
    "Calculate weighted sum\n",
    "$$y_2=w_{21}x_{1}+w_{22}x_{2}+w_{23}x_{3}+w_{24}x_{4}$$\n",
    "- - -\n",
    "### Enter Query, Key, Value\n",
    "Every input vector $i$ appears in three roles:\n",
    "- Calculate weights for its own output $w_{ij}$ -- *query*\n",
    "- Calculate weights for other output's weights $w_{zj}$, where $j=i$ -- *key*\n",
    "- Part of the weighted sum to calculate each $y_i$ -- *value*\n",
    "\n",
    "In the case of $x_2$, the above section showed where it played a role in computing weight matrices $w_{2j}$ and output $y_2$. If we were to demonstrate equations for computing $y_{1}, y_{3}, y_{4}$, input vector $x_i$ would play a role in computing $w_{12}, w_{32}, w_{42}$ in the three different computations separately, as well as in the relevant weighted sums. \n",
    "\n",
    "**Query:**\n",
    "- $x_{i}$ appears in calculating weights for its own output $y_i$\n",
    "- $x_2$ appears in calculating weights for its own output $y_{2}$\n",
    "\n",
    "**Key:**\n",
    "- $x_i$ appears in calculating weights for all outputs\n",
    "- $x_2$ appears in calculating weights for outputs $y_1, y_2, y_3, y_4$\n",
    "\n",
    "**Value:**\n",
    "- $x_i$ appears in the weighted sum for all outputs\n",
    "- $x_2$ appears in the weighted sum for $y_1, y_2, y_3, y_4$\n",
    "\n",
    "**$W_{q}, W_{k}, W_{v}$**<br>\n",
    "In practice we have additional trainable weight matrices $W_{q}, W_{k} \\quad \\text{and} \\quad W_{v}$ each for query, key and value. These are $d \\times d$ matrices and can be multiplied by our $d \\times 1$ input vector $x_{i}$. \n",
    "\n",
    "Hence our $d \\times 1$ vector $x_{i}$ gets transformed into three separate $d \\times 1$ vectors:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_{q}x_{i} = q_{i}\\\\\\\\\n",
    "W_{k}x_{i} = k_{i}\\\\\\\\\n",
    "W_{v}x_{i} = v_{i},\n",
    "\\end{aligned}\n",
    "$$\n",
    "which are used to compute the weight matrices $w'_{ij}$.\n",
    "\n",
    "Note that in the above section $w'_{ij}=x'_{i}x_{j}$, where $x_{i}$ played the role of query - *i.e.* helped compute weights for its own output and $x_{j}$ played the role of key - *i.e.* helped compute weights for other input's output. \n",
    "\n",
    "Now using the relevant query and key vectors, we compute:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w'_{ij} = \\frac{(W_{q}x_{i})'W_{k}x_{j}}{\\sqrt{d}}\\\\\\\\\n",
    "= \\frac{q'_ik_j}{\\sqrt{d}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $w'_{ij}$ will be $1 \\times 1$ scalar, as we multiply $(d \\times 1)' (d \\times 1) = (1 \\times d)(d \\times 1)$.\n",
    "\n",
    "From here on, weight matrix $w_{ij}$ is computed as before:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_{ij} = softmax(w'_{ij}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "but again the final output computation changes from its previous form of $y_i = \\sum_{j=1}^{t}w_{ij}x_{j}$ to\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{i} = \\sum_{j=1}^{t}w_{ij}W_{v}x_{j}\\\\\\\\\n",
    "= \\sum_{j=1}^{t}w_{ij}v_{j},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $y_i$ will also be of dimension $d \\times 1$ as $w_{ij}$ is a scalar and $v_{j}$ is of dimension $d_{1}.$\n",
    "\n",
    "**Back to Specific Case - Compute $y_{2}$:**<br>\n",
    "**Step 1)**<br>\n",
    "Obtain query vector $q_2$ using $x_2$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{2}=W_{q}x_2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "Transpose of $q_2$ will play the role of query in this case, instead of $x'_2$ used previously.\n",
    "\n",
    "**Step 2)**<br>\n",
    "Obtain key vectors $k_1, k_2, k_3, k_4$ using $x_1, x_2, x_3, x_4$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "k_1 = W_kx_1\\\\\\\\\n",
    "k_2=W_kx_2\\\\\\\\\n",
    "k_3=W_kx_3\\\\\\\\\n",
    "k_4=W_kx_4\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Step 3)**<br>\n",
    "Compute matrices $w'_{ij}: w'_{21}, \\quad w'_{22}, \\quad w'_{23}, \\quad w'_{24}$<br>\n",
    "*Note that these matrices are exactly what we call attention scores.*\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w'_{21} = \\frac{q'_2k_1}{\\sqrt{d}}\\\\\\\\\n",
    "w'_{22} = \\frac{q'_2k_2}{\\sqrt{d}}\\\\\\\\\n",
    "w'_{23} = \\frac{q'_2k_3}{\\sqrt{d}}\\\\\\\\\n",
    "w'_{24} = \\frac{q'_2k_4}{\\sqrt{d}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Step 4)**<br>\n",
    "Normalize weight matrices (attention scores) using $softmax$\n",
    "$$\\begin{aligned}\n",
    "w_{21}=\\frac{exp(w'_{21})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{22}=\\frac{exp(w'_{22})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{23}=\\frac{exp(w'_{23})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "w_{24}=\\frac{exp(w'_{24})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\\\\\n",
    "\\end{aligned}$$\n",
    "**Step 5)**<br>\n",
    "Obtain value vectors $v_1, v_2, v_3, v_4$ which will be used in the weighted sum to compute $y_2$ instead of previously used $x_1, x_2, x_3, x_4$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_1 = W_{v}x_1\\\\\\\\\n",
    "v_2 = W_{v}x_2\\\\\\\\\n",
    "v_3 = W_{v}x_3\\\\\\\\\n",
    "v_4 = W_{v}x_4\n",
    "\\end{aligned}\n",
    "$$\n",
    "**Step 6)**<br>\n",
    "Calculate weighted sum:\n",
    "$$\n",
    "\\begin{align}\n",
    "y_2 = w_{21}v_1 + w_{22}v_2 + w_{23}v_3 + w_{24}v_4\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "np.random.seed(66)\n",
    "tf.random.set_seed(66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of the $y_2$ Calculation in Python\n",
    "Since in the above example have four vectors in total, we imagine we had a four-word-long sentence that we embedded as $d \\times 1$ vectors $x_1, x_2, x_3, x_4$. To make things a bit more confusing, we can let $d=1$, meaning our embedding dimension is $4$ and $x_i$ is a vector of $4 \\times 1$ dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0): Generate $x_1, x_2, x_3, x_4$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vectors are:\n",
      "x_1:\n",
      "[[-0.5901138 ]\n",
      " [ 0.10746985]\n",
      " [-0.68417746]\n",
      " [-1.087325  ]]\n",
      "x_2:\n",
      "[[ 0.06799816]\n",
      " [ 0.26192757]\n",
      " [ 2.078989  ]\n",
      " [-0.15885977]]\n",
      "x_3:\n",
      "[[-0.4177143 ]\n",
      " [-0.6695858 ]\n",
      " [-0.05910626]\n",
      " [ 1.4612687 ]]\n",
      "x_4:\n",
      "[[-0.27432463]\n",
      " [-0.44688827]\n",
      " [-0.7900979 ]\n",
      " [-0.03789218]]\n",
      "With shapes: (4, 1), (4, 1), (4, 1), (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define embedding dimension\n",
    "d = 4\n",
    "# Randomly initialize embedded vectors\n",
    "x_1 = tf.random.normal((d,1))\n",
    "x_2 = tf.random.normal((d,1))\n",
    "x_3 = tf.random.normal((d,1))\n",
    "x_4 = tf.random.normal((d,1))\n",
    "\n",
    "print(f'Our vectors are:\\n'\n",
    "      f'x_1:\\n{x_1}\\n'\n",
    "      f'x_2:\\n{x_2}\\n'\n",
    "      f'x_3:\\n{x_3}\\n'\n",
    "      f'x_4:\\n{x_4}\\n'\n",
    "      f'With shapes: {x_1.shape}, {x_2.shape}, {x_3.shape}, {x_4.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0): Generate $W_q, W_k, W_v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our matrices are:\n",
      "W_q:\n",
      "[[-1.3717711  -0.98146987  0.8260041  -1.2923543 ]\n",
      " [ 0.30707964  0.786002   -2.103323   -1.7300906 ]\n",
      " [-0.5035843  -0.4742107  -0.2712986  -0.63562334]\n",
      " [ 1.2032156   0.18251911 -1.4129548   1.0945445 ]]\n",
      "W_k:\n",
      "[[-1.6024884  -1.1137156  -0.04244602  0.34064332]\n",
      " [-1.2472221   0.23090297 -0.01369113  0.97045213]\n",
      " [ 0.4897503   0.4532431  -0.56273043  0.02543709]\n",
      " [-1.9602743  -1.1043301  -0.9142158   0.13891058]]\n",
      "W_v:\n",
      "[[ 4.4455358e-01 -7.1996415e-01  9.7427595e-01 -7.3896867e-01]\n",
      " [ 2.7865940e-01 -1.2198892e+00 -4.6462238e-01  1.2124052e+00]\n",
      " [ 1.2106689e+00  1.3768704e+00 -2.1070539e-04  4.4651002e-01]\n",
      " [ 9.5693940e-01  1.3449348e+00  1.0805776e+00 -5.2276546e-01]]\n",
      "With shapes: (4, 4), (4, 4), (4, 4)\n"
     ]
    }
   ],
   "source": [
    "W_q = tf.random.normal((d,d))\n",
    "W_k = tf.random.normal((d,d))\n",
    "W_v = tf.random.normal((d,d))\n",
    "\n",
    "print(f'Our matrices are:\\n'\n",
    "      f'W_q:\\n{W_q}\\n'\n",
    "      f'W_k:\\n{W_k}\\n'\n",
    "      f'W_v:\\n{W_v}\\n'\n",
    "      f'With shapes: {W_q.shape}, {W_k.shape}, {W_v.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1): Obtain query vector $q_2$ using $x_2$**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{2}=W_{q}x_2,\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_2:\n",
      "[[ 1.5722046]\n",
      " [-3.8711874]\n",
      " [-0.6215035]\n",
      " [-2.9817736]]\n",
      "With shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Multiply 4x4 W_q with 4x1 x_2\n",
    "q_2 = tf.matmul(W_q, x_2)\n",
    "\n",
    "print(f'q_2:\\n{q_2}\\n'\n",
    "      f'With shape: {q_2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2) Obtain key vectors $k_1, k_2, k_3, k_4$ using $x_1, x_2, x_3, x_4$**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "k_1 = W_kx_1\\\\\n",
    "k_2=W_kx_2\\\\\n",
    "k_3=W_kx_3\\\\\n",
    "k_4=W_kx_4\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our key vectors are:\n",
      "k_1:\n",
      "[[ 0.48461038]\n",
      " [-0.28501165]\n",
      " [ 0.11705065]\n",
      " [ 1.5125477 ]]\n",
      "k_2:\n",
      "[[-0.5430384 ]\n",
      " [-0.20695846]\n",
      " [-1.0219324 ]\n",
      " [-2.3452613 ]]\n",
      "k_3:\n",
      "[[ 1.9153907 ]\n",
      " [ 1.7852738 ]\n",
      " [-0.43762955]\n",
      " [ 1.8153    ]]\n",
      "k_4:\n",
      "[[0.95793724]\n",
      " [0.21300066]\n",
      " [0.10674866]\n",
      " [1.7483201 ]]\n",
      "With shapes: (4, 1), (4, 1), (4, 1), (4, 1)\n"
     ]
    }
   ],
   "source": [
    "k_1 = tf.matmul(W_k, x_1)\n",
    "k_2 = tf.matmul(W_k, x_2)\n",
    "k_3 = tf.matmul(W_k, x_3)\n",
    "k_4 = tf.matmul(W_k, x_4)\n",
    "\n",
    "print(f'Our key vectors are:\\n'\n",
    "      f'k_1:\\n{k_1}\\n'\n",
    "      f'k_2:\\n{k_2}\\n'\n",
    "      f'k_3:\\n{k_3}\\n'\n",
    "      f'k_4:\\n{k_4}\\n'\n",
    "      f'With shapes: {k_1.shape}, {k_2.shape}, {k_3.shape}, {k_4.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3) Compute attention scores $w'_{ij}: w'_{21}, \\quad w'_{22}, \\quad w'_{23}, \\quad w'_{24}$**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w'_{21} = \\frac{q'_2k_1}{\\sqrt{d}}\\\\\\\\\n",
    "w'_{22} = \\frac{q'_2k_2}{\\sqrt{d}}\\\\\\\\\n",
    "w'_{23} = \\frac{q'_2k_3}{\\sqrt{d}}\\\\\\\\\n",
    "w'_{24} = \\frac{q'_2k_4}{\\sqrt{d}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our attention scores are:\n",
      "w_T_21:\n",
      "[[-1.3587911]]\n",
      "w_T_22:\n",
      "[[3.78779]]\n",
      "w_T_23:\n",
      "[[-4.5202847]]\n",
      "w_T_24:\n",
      "[[-2.2989657]]\n",
      "With shapes: (1, 1), (1, 1), (1, 1), (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define scaling factor - square root of d \n",
    "scale_factor = tf.math.sqrt(float(d))\n",
    "\n",
    "# Transpose q_2\n",
    "q_T_2 = tf.transpose(q_2)\n",
    "\n",
    "# Obtain attention scores\n",
    "w_T_21 = tf.matmul(q_T_2, k_1) / scale_factor\n",
    "w_T_22 = tf.matmul(q_T_2, k_2) / scale_factor\n",
    "w_T_23 = tf.matmul(q_T_2, k_3) / scale_factor\n",
    "w_T_24 = tf.matmul(q_T_2, k_4) / scale_factor\n",
    "\n",
    "print(f'Our attention scores are:\\n'\n",
    "      f'w_T_21:\\n{w_T_21}\\n'\n",
    "      f'w_T_22:\\n{w_T_22}\\n'\n",
    "      f'w_T_23:\\n{w_T_23}\\n'\n",
    "      f'w_T_24:\\n{w_T_24}\\n'\n",
    "      f'With shapes: {w_T_21.shape}, {w_T_22.shape}, {w_T_23.shape}, {w_T_24.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4) Normalize attention scores using $softmax$**\n",
    "$$\\begin{aligned}\n",
    "w_{21}=\\frac{exp(w'_{21})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\n",
    "w_{22}=\\frac{exp(w'_{22})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\n",
    "w_{23}=\\frac{exp(w'_{23})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\n",
    "w_{24}=\\frac{exp(w'_{24})}{exp(w'_{21})+exp(w'_{22})+exp(w'_{23})+exp(w'_{24})}\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our normalized attention scores are:\n",
      "w_21:\n",
      "[[0.00577114]]\n",
      "w_22:\n",
      "[[0.99173045]]\n",
      "w_23:\n",
      "[[0.00024448]]\n",
      "w_24:\n",
      "[[0.00225398]]\n",
      "With shapes: (1, 1), (1, 1), (1, 1), (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Take exponentials\n",
    "exp_w_T_21 = tf.exp(w_T_21)\n",
    "exp_w_T_22 = tf.exp(w_T_22)\n",
    "exp_w_T_23 = tf.exp(w_T_23)\n",
    "exp_w_T_24 = tf.exp(w_T_24)\n",
    "\n",
    "# Calculate sum\n",
    "exp_sum = exp_w_T_21 + exp_w_T_22 + exp_w_T_23 + exp_w_T_24\n",
    "\n",
    "# Calculate normalized scores\n",
    "w_21 = exp_w_T_21 / exp_sum\n",
    "w_22 = exp_w_T_22 / exp_sum\n",
    "w_23 = exp_w_T_23 / exp_sum\n",
    "w_24 = exp_w_T_24 / exp_sum\n",
    "\n",
    "print(f'Our normalized attention scores are:\\n'\n",
    "      f'w_21:\\n{w_21}\\n'\n",
    "      f'w_22:\\n{w_22}\\n'\n",
    "      f'w_23:\\n{w_23}\\n'\n",
    "      f'w_24:\\n{w_24}\\n'\n",
    "      f'With shapes: {w_21.shape}, {w_22.shape}, {w_23.shape}, {w_24.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5) Obtain value vectors $v_1, v_2, v_3, v_4$** \n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_1 = W_{v}x_1\\\\\n",
    "v_2 = W_{v}x_2\\\\\n",
    "v_3 = W_{v}x_3\\\\\n",
    "v_4 + W_{v}x_4\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our key vectors are:\n",
      "v_1:\n",
      "[[-0.2027902 ]\n",
      " [-1.2959363 ]\n",
      " [-1.0518178 ]\n",
      " [-0.59105414]]\n",
      "v_2:\n",
      "[[ 1.9845519 ]\n",
      " [-1.4591215 ]\n",
      " [ 0.37159303]\n",
      " [ 2.746901  ]]\n",
      "v_3:\n",
      "[[-0.84103614]\n",
      " [ 2.4995322 ]\n",
      " [-0.775163  ]\n",
      " [-2.1280463 ]]\n",
      "v_4:\n",
      "[[-0.54198074]\n",
      " [ 0.7898675 ]\n",
      " [-0.9641763 ]\n",
      " [-1.697501  ]]\n",
      "With shapes: (4, 1), (4, 1), (4, 1), (4, 1)\n"
     ]
    }
   ],
   "source": [
    "v_1 = tf.matmul(W_v, x_1)\n",
    "v_2 = tf.matmul(W_v, x_2)\n",
    "v_3 = tf.matmul(W_v, x_3)\n",
    "v_4 = tf.matmul(W_v, x_4)\n",
    "\n",
    "print(f'Our key vectors are:\\n'\n",
    "      f'v_1:\\n{v_1}\\n'\n",
    "      f'v_2:\\n{v_2}\\n'\n",
    "      f'v_3:\\n{v_3}\\n'\n",
    "      f'v_4:\\n{v_4}\\n'\n",
    "      f'With shapes: {v_1.shape}, {v_2.shape}, {v_3.shape}, {v_4.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6) Calculate weighted sum $y_2$**\n",
    "$$\n",
    "\\begin{align}\n",
    "y_2 = w_{21}v_1 + w_{22}v_2 + w_{23}v_3 + w_{24}v_4\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our output vector:\n",
      "y_2:\n",
      "[[ 1.965543  ]\n",
      " [-1.4521428 ]\n",
      " [ 0.36008716]\n",
      " [ 2.716428  ]]\n",
      "With shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# * operator since attention weights are scalars\n",
    "y_2 = w_21 * v_1 + w_22 * v_2 + w_23 * v_3 + w_24 * v_4\n",
    "\n",
    "print(f'Our output vector:\\n'\n",
    "      f'y_2:\\n{y_2}\\n'\n",
    "      f'With shape: {y_2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anbs_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
