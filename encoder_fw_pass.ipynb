{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers: Walking Through the Encoder Forward Pass\n",
    "\n",
    "**Disclaimer:**<br>\n",
    "\n",
    "The [notebook](https://github.com/anasashb/transformers_math_walkthrough/blob/main/transformer_attn_walkthrough.ipynb) on the math behind attention used conventional representation of input and output vectors $x_i$ and $y_i$ as column vectors. While this is a common representation in mathematical literature, in practical computations, including when dealing with TensorFlow, it is standard practice to treat these vectors as rows in the data matrix $X$. Therefore if single embedded input was of shape $d \\times 1$ in the previous notebook, here it will be of $1 \\times d$. Therefore some of the matrix multiplication notations will be different in this case. This difference between conventional notation and implementation stems from the need to use, among others, mini-batches.\n",
    "- - -\n",
    "### The Encoder Forward Pass Revisited\n",
    "In this notebook I demonstrate the forward pass of an encoder with a **single-head scaled dot product self attention,** and thus notation is tailored to that (*i.e.* $d_{model}=d_q=d_k=d_v$)<br>\n",
    "Positional emebedding is ignored for the time being.<br>\n",
    "\n",
    "**Encoder Inputs and Outputs**\n",
    "- Input matrix $X \\in \\mathbb{R}^{t \\times d}$\n",
    "- Output matrix $Y \\in \\mathbb{R}^{t \\times d}$<br>\n",
    "\n",
    "where $t$ stands for number of tokens and $d$ the embedding dimension.\n",
    "\n",
    "**Initialized, Trainable Weight Matrices**\n",
    "- Weight matrices for Query, Key and Value $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d}$\n",
    "\n",
    "**Step 1)** Obtain Query, Key and Value Matrices\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q = XW^{Q}\\\\\n",
    "K = XW^{K}\\\\\n",
    "V = XW^{V},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $Q, K, V \\in \\mathbb{R}^{t \\times d}$.\n",
    "\n",
    "**Step 2)** Compute Scaled Attention Scores\n",
    "$$\n",
    "scaled\\_attention = \\frac{QK^{T}}{\\sqrt{d}}\n",
    "$$\n",
    "where $scaled\\_attention \\in \\mathbb{R}^{t \\times t}$\n",
    "\n",
    "**Step 3)** Compute Normalized Attention Scores\n",
    "$$\n",
    "normalized\\_attention = softmax(\\frac{QK^{T}}{\\sqrt{d}})\n",
    "$$\n",
    "where $normalized\\_attention \\in \\mathbb{R}^{t \\times t}$\n",
    "\n",
    "**Step 4)** Compute Attention Output $Y$\n",
    "$$\n",
    "Y = softmax(\\frac{QK^{T}}{\\sqrt{d}})V\n",
    "$$\n",
    "where $Y \\in \\mathbb{R}^{t \\times d}$\n",
    "\n",
    "**Step 5)** Residual Connection and Normalization\n",
    "$$\n",
    "O = LayerNorm(Y+X)\n",
    "$$\n",
    "where $O \\in \\mathbb{R}^{t \\times d}$\n",
    "\n",
    "**Step 6)** Position-Wise Feed-Forward Networks\n",
    "$$\n",
    "FFN(O) = max(0, OW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "where $O, FFN(O) \\in \\mathbb{R}^{t \\times d}$, and $OW_1 + b_1 \\in \\mathbb{R}^{t \\times 4d}$ \n",
    "\n",
    "**Step 7)** Residual Connection and Normalization\n",
    "$$\n",
    "encoder\\_output = LayerNorm(FFN(O)+O)\n",
    "$$\n",
    "where $encoder\\_output \\in \\mathbb{R}^{t \\times d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 20:00:58.223337: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-11 20:00:58.278232: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-11 20:00:58.278287: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-11 20:00:58.278323: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-11 20:00:58.291862: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Required library\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0)** Obtain encoder input $X$, denoted as `embeddings_input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input tokens are:\n",
      "['in', 'the', 'bleak', 'midwinter']\n",
      " with length: 4\n",
      "--------------------------------------------------------------------------------\n",
      "Our embeddings are:\n",
      "[[-0.0741843  -0.04915943  0.05049502 ... -0.01158285 -0.01054677\n",
      "   0.04119534]\n",
      " [ 0.00722885  0.04369759 -0.04274543 ... -0.02851543 -0.02436578\n",
      "   0.06748476]\n",
      " [ 0.04392974 -0.04113375 -0.00963057 ... -0.01584903 -0.03317151\n",
      "   0.02710428]\n",
      " [-0.08654431  0.06789926  0.09438287 ... -0.07203033 -0.01282948\n",
      "  -0.03549376]]\n",
      " with shape: (4, 300)\n"
     ]
    }
   ],
   "source": [
    "# Define input sentence\n",
    "sentence = 'in the bleak midwinter'\n",
    "\n",
    "# Tokenize\n",
    "input_tokens = sentence.split()\n",
    "print(f'Our input tokens are:\\n'\n",
    "      f'{input_tokens}\\n',\n",
    "      f'with length: {len(input_tokens)}')\n",
    "print('-'*80)\n",
    "\n",
    "# Pick embedding dimension\n",
    "d = 300\n",
    "\n",
    "# Generate embeddings randomly\n",
    "embeddings_input = tf.keras.initializers.RandomNormal()(shape=(len(input_tokens), d))\n",
    "print(f'Our embeddings are:\\n'\n",
    "      f'{embeddings_input}\\n',\n",
    "      f'with shape: {embeddings_input.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0)** Initialize $W_q, W_k, W_v$ matrices of $d \\times d$ dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our weight matrices are:\n",
      "W_q:\n",
      "[[ 0.03723269  0.09281119 -0.05361765 ... -0.05797908  0.01792317\n",
      "  -0.0382619 ]\n",
      " [ 0.02328759  0.03270123 -0.03892419 ... -0.00826736  0.04628843\n",
      "   0.01631828]\n",
      " [ 0.03692954 -0.01910817 -0.00831627 ...  0.03324584  0.10601654\n",
      "   0.08415358]\n",
      " ...\n",
      " [-0.01936804 -0.09737956  0.08661649 ... -0.00909938  0.0481543\n",
      "  -0.01972168]\n",
      " [-0.05319289  0.01023542 -0.042817   ...  0.08164191  0.0091115\n",
      "  -0.09240133]\n",
      " [ 0.00219032  0.06736977 -0.03829434 ...  0.02602117  0.00231728\n",
      "   0.02032198]]\n",
      " W_k:\n",
      "[[ 0.01603875 -0.00056137 -0.06327474 ... -0.03770884  0.00751227\n",
      "   0.05085857]\n",
      " [ 0.00794533  0.03619261 -0.07646581 ...  0.02190027 -0.09783776\n",
      "  -0.1076859 ]\n",
      " [ 0.00324819  0.02829494  0.07573404 ...  0.00340645  0.0515605\n",
      "   0.04967254]\n",
      " ...\n",
      " [-0.07645648 -0.03013856  0.03536862 ...  0.01988157  0.02923589\n",
      "   0.05864898]\n",
      " [-0.11877687  0.02494119  0.04692116 ... -0.14199881  0.08960991\n",
      "  -0.01894834]\n",
      " [-0.05679299 -0.0028196  -0.09659957 ... -0.00726818  0.03675446\n",
      "  -0.06242776]]\n",
      " W_v:\n",
      "[[-0.0153725  -0.02845365 -0.03760555 ... -0.0864379   0.00170164\n",
      "   0.06901576]\n",
      " [ 0.05388607 -0.01082442  0.03588552 ... -0.00483522 -0.05911912\n",
      "   0.03892165]\n",
      " [-0.08203886  0.06859865  0.01867569 ... -0.03900505 -0.00663228\n",
      "   0.06156681]\n",
      " ...\n",
      " [ 0.01363345  0.04435747  0.00553723 ...  0.00500523  0.14766115\n",
      "  -0.04141982]\n",
      " [-0.03816215 -0.01955364  0.03340584 ...  0.02844944 -0.03023493\n",
      "  -0.03228768]\n",
      " [-0.02627741  0.00855623  0.0108777  ... -0.10392808 -0.03869988\n",
      "  -0.01227269]]\n",
      " with shapes: (300, 300), (300, 300), (300, 300)\n"
     ]
    }
   ],
   "source": [
    "# Initialize query, key and value weight matrices randomly\n",
    "W_q = tf.keras.initializers.RandomNormal()(shape=(d, d))\n",
    "W_k = tf.keras.initializers.RandomNormal()(shape=(d, d))\n",
    "W_v = tf.keras.initializers.RandomNormal()(shape=(d, d))\n",
    "\n",
    "print(f'Our weight matrices are:\\n'\n",
    "      f'W_q:\\n{W_q}\\n',\n",
    "      f'W_k:\\n{W_k}\\n',\n",
    "      f'W_v:\\n{W_v}\\n',            \n",
    "      f'with shapes: {W_q.shape}, {W_k.shape}, {W_v.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1)** Obtain $Q, K, V$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q = XW^{Q}\\\\\n",
    "K = XW^{K}\\\\\n",
    "V = XW^{V},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $Q, K, V \\in \\mathbb{R}^{t \\times d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our matrices are:\n",
      "Query:\n",
      "[[ 0.02234076 -0.07591635 -0.09255555 ...  0.03983417  0.0057872\n",
      "   0.01784528]\n",
      " [-0.01952961  0.05058814 -0.08638044 ... -0.06978144 -0.09432264\n",
      "  -0.0087329 ]\n",
      " [-0.03929292  0.0156117  -0.00356103 ...  0.02536867  0.02940155\n",
      "  -0.02930875]\n",
      " [-0.02998801  0.06668639 -0.02769908 ...  0.03847094  0.023365\n",
      "  -0.10388693]]\n",
      " Key:\n",
      "[[-0.05925012  0.05007924  0.00738794 ... -0.0012     -0.02463339\n",
      "  -0.03889964]\n",
      " [ 0.03592325  0.0922419   0.01586427 ... -0.0051557  -0.10935269\n",
      "  -0.00841157]\n",
      " [-0.00770236 -0.10901807 -0.03415017 ... -0.01817019 -0.02409642\n",
      "   0.00808546]\n",
      " [-0.0191135   0.04928383 -0.00215943 ...  0.02107384 -0.02404883\n",
      "  -0.01796726]]\n",
      " Value:\n",
      "[[-0.05953827 -0.00528069 -0.04912516 ... -0.03327402  0.01761387\n",
      "  -0.02158837]\n",
      " [ 0.03896216 -0.00531136 -0.09674695 ... -0.06482318  0.03180076\n",
      "  -0.01549592]\n",
      " [-0.03189508 -0.03950288  0.01442864 ...  0.00566404  0.10965772\n",
      "   0.06029917]\n",
      " [ 0.00702286  0.02378504  0.03215706 ...  0.06221448  0.06543559\n",
      "  -0.01233201]]\n",
      " with shapes: (4, 300), (4, 300), (4, 300)\n"
     ]
    }
   ],
   "source": [
    "# Obtain Query, Key and Value matrices\n",
    "Q = tf.matmul(embeddings_input, W_q)\n",
    "K = tf.matmul(embeddings_input, W_k)\n",
    "V = tf.matmul(embeddings_input, W_v)\n",
    "\n",
    "print(f'Our matrices are:\\n'\n",
    "      f'Query:\\n{Q}\\n',\n",
    "      f'Key:\\n{K}\\n',\n",
    "      f'Value:\\n{V}\\n',            \n",
    "      f'with shapes: {Q.shape}, {K.shape}, {V.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2)** Compute Scaled Attention Scores\n",
    "$$\n",
    "scaled\\_attention = \\frac{QK^{T}}{\\sqrt{d}}\n",
    "$$\n",
    "where $scaled\\_attention \\in \\mathbb{R}^{t \\times t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled attention scores:\n",
      "[[ 0.00263519 -0.00174089 -0.00037623 -0.00035859]\n",
      " [ 0.00236044  0.00206439  0.00187134  0.00241708]\n",
      " [-0.00104838  0.00487237 -0.00182653  0.00039156]\n",
      " [ 0.00036275  0.00206183 -0.00057808  0.00099374]]\n",
      "with shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Compute raw attention scores QK'\n",
    "QK_T = tf.matmul(Q, tf.transpose(K))\n",
    "\n",
    "# Compute scaling factor (square root of d)\n",
    "scaling_factor = tf.sqrt(tf.cast(d, dtype=tf.float32))\n",
    "\n",
    "# Compute scaled attention\n",
    "scaled_QK_T = QK_T / scaling_factor\n",
    "\n",
    "print(f'Scaled attention scores:\\n'\n",
    "      f'{scaled_QK_T}\\n'\n",
    "      f'with shape: {scaled_QK_T.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3)** Normalize Scaled Attention Scores Using $softmax$\n",
    "$$\n",
    "normalized\\_attention = softmax(\\frac{QK^{T}}{\\sqrt{d}})\n",
    "$$\n",
    "where $normalized\\_attention \\in \\mathbb{R}^{t \\times t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized attention scores:\n",
      "[[0.25064936 0.24955489 0.24989569 0.24990009]\n",
      " [0.2500455  0.24997151 0.24992326 0.25005966]\n",
      " [0.24958809 0.25107023 0.24939394 0.24994774]\n",
      " [0.24991307 0.25033805 0.24967806 0.2500708 ]]\n",
      "with shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Squash through softmax\n",
    "norm_attention = tf.keras.activations.softmax(scaled_QK_T)\n",
    "\n",
    "print(f'Normalized attention scores:\\n'\n",
    "      f'{norm_attention}\\n'\n",
    "      f'with shape: {norm_attention.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4)** Obtain Weighted Sum $Y$, Denoted as `attention_output`\n",
    "$$\n",
    "Y = softmax(\\frac{QK^{T}}{\\sqrt{d}})V\n",
    "$$\n",
    "where $Y \\in \\mathbb{R}^{t \\times d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output:\n",
      "[[-0.01141546 -0.00657679 -0.02481516 ... -0.00755423  0.05610629\n",
      "   0.00270854]\n",
      " [-0.01136303 -0.00657311 -0.02482027 ... -0.00755106  0.05612237\n",
      "   0.00271481]\n",
      " [-0.0112769  -0.00655828 -0.02491533 ... -0.00761702  0.05608388\n",
      "   0.00267712]\n",
      " [-0.01133297 -0.00656441 -0.0248524  ... -0.00757111  0.05610554\n",
      "   0.00269707]]\n",
      "with shape: (4, 300)\n"
     ]
    }
   ],
   "source": [
    "# Obtain weighted sum\n",
    "attention_output = tf.matmul(norm_attention, V)\n",
    "\n",
    "print(f'Attention output:\\n'\n",
    "      f'{attention_output}\\n'\n",
    "      f'with shape: {attention_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5)** Obtain Final Attention Output $O$, Denoted as `attention_add_norm`\n",
    "$$\n",
    "\\begin{aligned}\n",
    "O = LayerNorm(Y+X)\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $O \\in \\mathbb{R}^{t \\times d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized output:\n",
      "[[-1.592812   -1.0233724   0.529074   ... -0.3254979   0.90814054\n",
      "   0.8765707 ]\n",
      " [-0.08818263  0.6591465  -1.2371346  ... -0.6665808   0.5619165\n",
      "   1.2582444 ]\n",
      " [ 0.63286376 -0.9073197  -0.655313   ... -0.442916    0.44614235\n",
      "   0.5778193 ]\n",
      " [-1.824939    1.2616862   1.4205737  ... -1.4706278   0.91158247\n",
      "  -0.56323004]]\n",
      "with shape: (4, 300)\n"
     ]
    }
   ],
   "source": [
    "# Residual Connection: Add Original Input to Attention Output Elementwise\n",
    "attention_add = attention_output + embeddings_input\n",
    "\n",
    "attention_add_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_add)\n",
    "\n",
    "print(f'Normalized output:\\n'\n",
    "      f'{attention_add_norm}\\n'\n",
    "      f'with shape: {attention_add_norm.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6)** Position-Wise Feed-Forward Networks\n",
    "$$\n",
    "FFN(O) = max(0, OW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "where $O, FFN(O) \\in \\mathbb{R}^{t \\times d}$, and $OW_1 + b_1 \\in \\mathbb{R}^{t \\times 4d}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN output:\n",
      "[[ 2.882871   -0.9661224  -2.249414   ... -0.62725556 -0.01618898\n",
      "   0.32342726]\n",
      " [ 0.37960598 -0.9368476  -0.34070563 ...  0.39195973 -1.0509952\n",
      "  -0.3859678 ]\n",
      " [ 1.303434    0.44110128 -0.51026565 ... -1.0850596   0.27255872\n",
      "  -0.57497585]\n",
      " [ 0.44652134  0.48340976 -0.3626876  ...  0.7292412  -0.19128858\n",
      "   2.5369987 ]]\n",
      "with shape: (4, 300)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Inner Layer Weights and Bias\n",
    "W_1 = tf.keras.initializers.RandomNormal()(shape=(d, 4*d))\n",
    "b_1 = tf.Variable(tf.zeros(4*d))\n",
    "\n",
    "# Initialize Output Weights and Bias\n",
    "# Project back to original shape\n",
    "W_2 = tf.keras.initializers.RandomNormal()(shape=(4*d, d))\n",
    "b_2 = tf.Variable(tf.zeros(d))\n",
    "\n",
    "# Compute First Linear Projection\n",
    "inner_projection = tf.matmul(attention_add_norm, W_1) + b_1\n",
    "\n",
    "# For Operation, ReLU\n",
    "inner_output = tf.keras.activations.relu(inner_projection)\n",
    "\n",
    "# Second Linear Projection\n",
    "ffn_output = tf.matmul(inner_output, W_2) + b_2\n",
    "\n",
    "print(f'FFN output:\\n'\n",
    "      f'{ffn_output}\\n'\n",
    "      f'with shape: {ffn_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7)** Residual Connection and Normalization\n",
    "$$\n",
    "encoder\\_output = LayerNorm(FFN(O)+O)\n",
    "$$\n",
    "where $encoder\\_output \\in \\mathbb{R}^{t \\times d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output:\n",
      "[[ 0.97223    -1.3499476  -1.1593652  ... -0.61585456  0.69033915\n",
      "   0.9084598 ]\n",
      " [ 0.26922503 -0.14316043 -1.0852363  ... -0.14092863 -0.29632384\n",
      "   0.6901091 ]\n",
      " [ 1.5002097  -0.27086684 -0.7864182  ... -1.0535685   0.602627\n",
      "   0.07491418]\n",
      " [-0.9377893   1.222774    0.74742466 ... -0.497149    0.513909\n",
      "   1.3809491 ]]\n",
      "with shape: (4, 300)\n"
     ]
    }
   ],
   "source": [
    "# Residual Connection: Add Final Attention Output to Feed-Forward Output Elementwise\n",
    "# Noramlize\n",
    "encoder_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ffn_output + attention_add_norm)\n",
    "\n",
    "print(f'Encoder output:\\n'\n",
    "      f'{encoder_output}\\n'\n",
    "      f'with shape: {encoder_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, the encoder can be pushed through an output head in case of an encoder-only model, or can be sent to the decoder. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ansb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
