{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers: Walking Through the Encoder Forward Pass\n",
    "\n",
    "**Disclaimer:**<br>\n",
    "\n",
    "The [notebook](https://github.com/anasashb/transformers_math_walkthrough/blob/main/transformer_attn_walkthrough.ipynb) on the math behind attention used conventional representation of input and output vectors $x_i$ and $y_i$ as column vectors. While this is a common representation in mathematical literature, in practical computations, including when dealing with TensorFlow, it is standard practice to treat these vectors as rows in the data matrix $X$. Therefore if single embedded input was of shape $d \\times 1$ in the previous notebook, here it will be of $1 \\times d$. Therefore some of the matrix multiplication notations will be different in this case. This difference between conventional notation and implementation stems from the need to use, among others, mini-batches.\n",
    "- - -\n",
    "### The Encoder Forward Pass Revisited\n",
    "In this notebook I demonstrate the forward pass of an encoder with a **single-head scaled dot product self attention,** and thus notation is tailored to that (*i.e.* $d_{model}=d_q=d_k=d_v$)<br>\n",
    "\n",
    "**Encoder Inputs and Outputs**\n",
    "- Input matrix $X \\in \\mathbb{R}^{t \\times d}$\n",
    "- Output matrix $Y \\in \\mathbb{R}^{t \\times d}$<br>\n",
    "\n",
    "where $t$ stands for number of tokens and $d$ the embedding dimension.\n",
    "\n",
    "**Initialized, Trainable Weight Matrices**\n",
    "- Weight matrices for Query, Key and Value $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d}$\n",
    "\n",
    "**Step 1)** Obtain Query, Key and Value Matrices\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q = XW^{Q}\\\\\n",
    "K = XW^{K}\\\\\n",
    "V = XW^{V},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $Q, K, V \\in \\mathbb{R}^{t \\times d}$.\n",
    "\n",
    "**Step 2)** Compute Scaled Attention Scores\n",
    "$$\n",
    "scaled\\_attention = \\frac{QK^{T}}{\\sqrt{d}}\n",
    "$$\n",
    "where $scaled\\_attention \\in \\mathbb{R}^{t \\times t}$\n",
    "\n",
    "**Step 3)** Compute Normalized Attention Scores\n",
    "$$\n",
    "normalized\\_attention = softmax(\\frac{QK^{T}}{\\sqrt{d}})\n",
    "$$\n",
    "where $normalized\\_attention \\in \\mathbb{R}^{t \\times t}$\n",
    "\n",
    "**Step 4)** Compute Encoder/Attention Output $Y$\n",
    "$$\n",
    "Y = softmax(\\frac{QK^{T}}{\\sqrt{d}})V\n",
    "$$\n",
    "where $Y \\in \\mathbb{R}^{t \\times d}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 17:40:23.111884: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-10 17:40:23.164329: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-10 17:40:23.164388: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-10 17:40:23.164419: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-10 17:40:23.176721: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "np.random.seed(66)\n",
    "tf.random.set_seed(66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0)** Obtain encoder input $X$, denoted as `embeddings_input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input tokens are:\n",
      "['in', 'the', 'bleak', 'midwinter']\n",
      " with length: 4\n",
      "--------------------------------------------------------------------------------\n",
      "Our embeddings are:\n",
      "[[-0.07616111 -0.01057373 -0.07564942 ... -0.03830564 -0.01908852\n",
      "   0.07599268]\n",
      " [-0.05880255 -0.04481804  0.03535214 ...  0.02076481 -0.01052421\n",
      "   0.00542679]\n",
      " [ 0.01348811  0.00810569 -0.00789542 ...  0.06691866  0.0353842\n",
      "  -0.02301901]\n",
      " [ 0.07057167  0.00093471 -0.03097019 ...  0.11004905  0.06634303\n",
      "   0.01168947]]\n",
      " with shape: (4, 300)\n"
     ]
    }
   ],
   "source": [
    "# Define input sentence\n",
    "sentence = 'in the bleak midwinter'\n",
    "\n",
    "# Tokenize\n",
    "input_tokens = sentence.split()\n",
    "print(f'Our input tokens are:\\n'\n",
    "      f'{input_tokens}\\n',\n",
    "      f'with length: {len(input_tokens)}')\n",
    "print('-'*80)\n",
    "\n",
    "# Pick embedding dimension\n",
    "d = 300\n",
    "\n",
    "# Generate embeddings randomly\n",
    "embeddings_input = tf.keras.initializers.RandomNormal()(shape=(len(input_tokens), d))\n",
    "print(f'Our embeddings are:\\n'\n",
    "      f'{embeddings_input}\\n',\n",
    "      f'with shape: {embeddings_input.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0)** Initialize $W_q, W_k, W_v$ matrices of $d \\times d$ dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our matrices are:\n",
      "W_q:\n",
      "[[ 0.04303246  0.03313142  0.03040164 ... -0.02537489  0.02974148\n",
      "   0.01310409]\n",
      " [-0.01867909  0.03220072 -0.03042476 ...  0.06480556 -0.07691052\n",
      "  -0.02612035]\n",
      " [-0.1046806   0.04067704 -0.10271883 ... -0.05716374 -0.0358577\n",
      "  -0.0737501 ]\n",
      " ...\n",
      " [ 0.07037354 -0.04791072 -0.02540406 ...  0.07900711  0.03343224\n",
      "  -0.00564269]\n",
      " [ 0.06309637  0.0914696  -0.07195995 ... -0.04517866  0.10747699\n",
      "  -0.0859692 ]\n",
      " [-0.03365593  0.06519532 -0.0437687  ...  0.07391986  0.0249158\n",
      "  -0.02816771]]\n",
      " W_k:\n",
      "[[-2.12770253e-02  3.11037432e-02  8.63985741e-04 ... -6.93682209e-02\n",
      "   4.09356281e-02 -3.21749225e-02]\n",
      " [ 1.76140741e-02 -2.15053409e-02  8.03746581e-02 ... -2.19596978e-02\n",
      "   6.19593496e-03  2.72714738e-02]\n",
      " [ 3.15739103e-02  9.11310688e-03  6.57749102e-02 ... -6.83886409e-02\n",
      "  -4.80998345e-02  6.35050014e-02]\n",
      " ...\n",
      " [-3.73952799e-02  6.28292412e-02  1.24926805e-01 ...  2.69469172e-02\n",
      "  -1.18080636e-04 -1.10481749e-03]\n",
      " [-9.17510837e-02 -4.51365411e-02  5.76580549e-03 ... -4.63957265e-02\n",
      "  -6.14206493e-02 -1.09246135e-01]\n",
      " [-2.50094086e-02 -6.02163672e-02  3.27700041e-02 ...  3.14908810e-02\n",
      "  -3.81369926e-02  4.00853828e-02]]\n",
      " W_v:\n",
      "[[-0.07306407 -0.00861072 -0.00226419 ... -0.04389615  0.08310363\n",
      "   0.0491641 ]\n",
      " [ 0.0446938  -0.03329364  0.03301819 ...  0.08776995 -0.02486587\n",
      "  -0.0290009 ]\n",
      " [ 0.05985961  0.00898911 -0.01183303 ... -0.04738879 -0.11345951\n",
      "   0.01698691]\n",
      " ...\n",
      " [-0.00374574 -0.04645802 -0.00032894 ...  0.03957962 -0.0048358\n",
      "  -0.05134995]\n",
      " [ 0.0450274   0.06242932  0.02201977 ...  0.05589038  0.02399052\n",
      "   0.08831485]\n",
      " [-0.0157293  -0.01432294 -0.00679338 ...  0.03482438 -0.06083642\n",
      "   0.00521266]]\n",
      " with shapes: (300, 300), (300, 300), (300, 300)\n"
     ]
    }
   ],
   "source": [
    "# Initialize query, key and value weight matrices randomly\n",
    "W_q = tf.keras.initializers.RandomNormal()(shape=(d, d))\n",
    "W_k = tf.keras.initializers.RandomNormal()(shape=(d, d))\n",
    "W_v = tf.keras.initializers.RandomNormal()(shape=(d, d))\n",
    "\n",
    "print(f'Our weight matrices are:\\n'\n",
    "      f'W_q:\\n{W_q}\\n',\n",
    "      f'W_k:\\n{W_k}\\n',\n",
    "      f'W_v:\\n{W_v}\\n',            \n",
    "      f'with shapes: {W_q.shape}, {W_k.shape}, {W_v.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1)** Obtain $Q, K, V$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q = XW^{Q}\\\\\n",
    "K = XW^{K}\\\\\n",
    "V = XW^{V},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $Q, K, V \\in \\mathbb{R}^{t \\times d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our matrices are:\n",
      "Query:\n",
      "[[-0.01358831  0.00225072  0.04069463 ... -0.04041561  0.07240771\n",
      "   0.02474956]\n",
      " [ 0.00899658  0.01655877 -0.00674951 ... -0.04415486 -0.0130793\n",
      "  -0.07043502]\n",
      " [ 0.04078966  0.062748    0.01114449 ...  0.03822777  0.06681118\n",
      "  -0.04799606]\n",
      " [ 0.07740284  0.02113403  0.00226755 ... -0.02833924 -0.04025527\n",
      "   0.0632926 ]]\n",
      " Key:\n",
      "[[-0.00755814 -0.01416114  0.03085505 ...  0.02321012 -0.02985067\n",
      "  -0.06097139]\n",
      " [ 0.02322749 -0.04437843 -0.03968051 ...  0.03866518  0.03314797\n",
      "   0.01338449]\n",
      " [-0.09906237  0.01016132  0.03210665 ...  0.03507121 -0.02283784\n",
      "   0.13048255]\n",
      " [-0.0725933   0.04244903  0.02024318 ... -0.0377757   0.00357432\n",
      "   0.00459064]]\n",
      " Value:\n",
      "[[-0.0428874  -0.0227301  -0.05851181 ...  0.0111142   0.07013445\n",
      "   0.05753509]\n",
      " [ 0.00068681  0.00997287  0.00854428 ...  0.05659451  0.07158233\n",
      "  -0.04577174]\n",
      " [-0.00587513 -0.02620863 -0.0323611  ...  0.02873419  0.00258767\n",
      "   0.02111014]\n",
      " [-0.0233941  -0.05579733  0.01938301 ... -0.02305271  0.02037075\n",
      "   0.00254119]]\n",
      " with shapes: (4, 300), (4, 300), (4, 300)\n"
     ]
    }
   ],
   "source": [
    "# Obtain Query, Key and Value matrices\n",
    "\n",
    "Q = tf.matmul(embeddings_input, W_q)\n",
    "K = tf.matmul(embeddings_input, W_k)\n",
    "V = tf.matmul(embeddings_input, W_v)\n",
    "\n",
    "print(f'Our matrices are:\\n'\n",
    "      f'Query:\\n{Q}\\n',\n",
    "      f'Key:\\n{K}\\n',\n",
    "      f'Value:\\n{V}\\n',            \n",
    "      f'with shapes: {Q.shape}, {K.shape}, {V.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2)** Compute Scaled Attention Scores\n",
    "$$\n",
    "scaled\\_attention = \\frac{QK^{T}}{\\sqrt{d}}\n",
    "$$\n",
    "where $scaled\\_attention \\in \\mathbb{R}^{t \\times t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled attention:\n",
      "[[-1.2512440e-03  2.6803457e-03 -2.3253488e-03  4.8317036e-04]\n",
      " [ 2.1728969e-04  4.1843363e-04 -1.0025405e-05  2.6821145e-03]\n",
      " [-1.5776812e-03 -6.6248595e-04 -8.4195595e-04 -1.9943712e-03]\n",
      " [-4.6782996e-04 -2.2011064e-03  4.2129325e-04 -1.6234732e-03]]\n",
      "with shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Compute raw attention scores QK'\n",
    "QK_T = tf.matmul(Q, tf.transpose(K))\n",
    "\n",
    "# Compute scaling factor (square root of d)\n",
    "scaling_factor = tf.sqrt(tf.cast(d, dtype=tf.float32))\n",
    "\n",
    "# Compute scaled attention\n",
    "scaled_QK_T = QK_T / scaling_factor\n",
    "\n",
    "print(f'Scaled attention:\\n'\n",
    "      f'{scaled_QK_T}\\n'\n",
    "      f'with shape: {scaled_QK_T.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3)** Normalize Scaled Attention Scores Using $softmax$\n",
    "$$\n",
    "normalized\\_attention = softmax(\\frac{QK^{T}}{\\sqrt{d}})\n",
    "$$\n",
    "where $normalized\\_attention \\in \\mathbb{R}^{t \\times t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self attention scores:\n",
      "[[0.24971272 0.25069642 0.24944463 0.2501462 ]\n",
      " [0.24984749 0.24989775 0.2497907  0.25046408]\n",
      " [0.24992284 0.25015166 0.25010678 0.24981873]\n",
      " [0.25012487 0.24969172 0.25034738 0.24983601]]\n",
      "with shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Squash through softmax\n",
    "attention = tf.keras.activations.softmax(scaled_QK_T)\n",
    "\n",
    "print(f'Self attention scores:\\n'\n",
    "      f'{attention}\\n'\n",
    "      f'with shape: {attention.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4)** Obtain Weighted Sum $Y$, Denoted as `encoder_output`\n",
    "$$\n",
    "Y = softmax(\\frac{QK^{T}}{\\sqrt{d}})V\n",
    "$$\n",
    "where $Y \\in \\mathbb{R}^{t \\times d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output:\n",
      "[[-0.01785482 -0.02367092 -0.01569284 ...  0.01836444  0.04120005\n",
      "   0.00879391]\n",
      " [-0.01787061 -0.02370876 -0.01571258 ...  0.01832335  0.0411597\n",
      "   0.00884634]\n",
      " [-0.01786043 -0.02368021 -0.01573756 ...  0.01836252  0.04117083\n",
      "   0.00884408]\n",
      " [-0.01787123 -0.02369666 -0.01576076 ...  0.01834525  0.04115305\n",
      "   0.00888188]]\n",
      "with shape: (4, 300)\n"
     ]
    }
   ],
   "source": [
    "# Obtain weighted sum\n",
    "encoder_output = tf.matmul(attention, V)\n",
    "\n",
    "print(f'Encoder output:\\n'\n",
    "      f'{encoder_output}\\n'\n",
    "      f'with shape: {encoder_output.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ansb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
